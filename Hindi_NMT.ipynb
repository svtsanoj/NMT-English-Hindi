{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hindi_NMT_(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJCS4SCoWFaK",
        "colab_type": "text"
      },
      "source": [
        "# **Neural Machine Translator for English to Hindi**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "###This will be a Seq to Seq architecture without attention mechanisam.\n",
        "\n",
        "###I have tried several variation and mainly the notebook is split into 2 \n",
        "\n",
        "- ### **Unidirectional LSTM Model**\n",
        "- ### **Bidirectional LSTM Model**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxIfvlbvXGGJ",
        "colab_type": "code",
        "outputId": "b9fa2efd-d432-4275-a333-a4e22d6352e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!git clone https://github.com/svtsanoj/NMT-English-Hindi.git # for dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NMT-English-Hindi'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects:  16% (1/6)\u001b[K\rremote: Counting objects:  33% (2/6)\u001b[K\rremote: Counting objects:  50% (3/6)\u001b[K\rremote: Counting objects:  66% (4/6)\u001b[K\rremote: Counting objects:  83% (5/6)\u001b[K\rremote: Counting objects: 100% (6/6)\u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLac6WvZM9cy",
        "colab_type": "code",
        "outputId": "7fc78d89-b733-4792-ed52-c8af54b591f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from keras.layers import Input, LSTM, Embedding, Dense, Bidirectional\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9OFGXwGaF-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = \"/content/NMT-English-Hindi/Hi-En-Parallel_Corpus.xlsx\"\n",
        "\n",
        "dataframe = pd.read_excel(filepath)\n",
        "dataframe.to_csv('csvfile.csv', encoding='utf-8', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSLOU6CQaVbC",
        "colab_type": "code",
        "outputId": "9fb4b25b-59b0-4911-eae8-7a42571142fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df = pd.read_csv(\"/content/csvfile.csv\")\n",
        "df.columns = [\"en\", \"hi\"]\n",
        "print(df[\"hi\"][1], df['en'][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी, I'd like to tell you about one such child,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58CJzNDKX0On",
        "colab_type": "text"
      },
      "source": [
        "##Data Preprocessing\n",
        "I have initially converted the corpus from excel format to pandas to explore the dataset more easily. Later on i have converted it into a list.\n",
        "\n",
        "A simple regex command is used here.\n",
        "I have obtained all the punctuations and added it for replacement, additionally we can see that Hindi has its own numbers apart from 0-9, so as these numbers are one after the other in ascii we can use **०-९** to define the element between these as well.\n",
        "\n",
        "Note that input should  be converted to String or else the dataset containing different types might throw error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjVO33W_vtyq",
        "colab_type": "code",
        "outputId": "83944d37-8279-4790-dbd6-8e7ceb48f0f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "def preprocess(hi, en):\n",
        "\n",
        "  for i in range(len(hi)):\n",
        "    # Remove all the punctuations and digits. \n",
        "    # Hinid has characters for its own digits so we'll remove those as well as.\n",
        "    hi[i] = re.sub(\"[!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~0-9०-९]\", \"\", str(hi[i])).lower() # use str(input) as some chars maybe of diferent dtype which throwed an error for me\n",
        "    en[i] = re.sub(\"[!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~0-9०-९]\", \"\", str(en[i])).lower()\n",
        "    \n",
        "    hi[i] = re.sub(\"\\s+\", \" \", hi[i]) #replace extra white spaces with single white space\n",
        "    en[i] = re.sub(\"\\s+\", \" \", en[i])\n",
        "\n",
        "    hi[i] = \"<sos> \" + str(hi[i]) + \" <eos>\" # we add the start and end of sentence tag to the target language\n",
        "                                              # the input language does not require these tags\n",
        "  return zip(en,hi)\n",
        "\n",
        "df = pd.DataFrame(preprocess(df[\"hi\"][:], df[\"en\"][:]))\n",
        "df.columns = [\"en\", \"hi\"]\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>hi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>politicians do not have permission to do what ...</td>\n",
              "      <td>&lt;sos&gt; राजनीतिज्ञों के पास जो कार्य करना चाहिए ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id like to tell you about one such child</td>\n",
              "      <td>&lt;sos&gt; मई आपको ऐसे ही एक बच्चे के बारे में बतान...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this percentage is even greater than the perce...</td>\n",
              "      <td>&lt;sos&gt; यह प्रतिशत भारत में हिन्दुओं प्रतिशत से ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what we really mean is that theyre bad at not ...</td>\n",
              "      <td>&lt;sos&gt; हम ये नहीं कहना चाहते कि वो ध्यान नहीं द...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the ending portion of these vedas is called up...</td>\n",
              "      <td>&lt;sos&gt; इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127602</th>\n",
              "      <td>examples of art deco construction can be found...</td>\n",
              "      <td>&lt;sos&gt; आर्ट डेको शैली के निर्माण मैरीन ड्राइव औ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127603</th>\n",
              "      <td>and put it in our cheeks</td>\n",
              "      <td>&lt;sos&gt; और अपने गालों में डाल लेते हैं। &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127604</th>\n",
              "      <td>as for the other derivatives of sulphur the co...</td>\n",
              "      <td>&lt;sos&gt; जहां तक गंधक के अन्य उत्पादों का प्रश्न ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127605</th>\n",
              "      <td>its complicated functioning is defined thus in...</td>\n",
              "      <td>&lt;sos&gt; zरचनाप्रकिया को उसने एक पहेली में यों बा...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127606</th>\n",
              "      <td>theyve just won four government contracts to b...</td>\n",
              "      <td>&lt;sos&gt; हाल ही में उन्हें सरकारी ठेका मिला है कर...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>127607 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       en                                                 hi\n",
              "0       politicians do not have permission to do what ...  <sos> राजनीतिज्ञों के पास जो कार्य करना चाहिए ...\n",
              "1                id like to tell you about one such child  <sos> मई आपको ऐसे ही एक बच्चे के बारे में बतान...\n",
              "2       this percentage is even greater than the perce...  <sos> यह प्रतिशत भारत में हिन्दुओं प्रतिशत से ...\n",
              "3       what we really mean is that theyre bad at not ...  <sos> हम ये नहीं कहना चाहते कि वो ध्यान नहीं द...\n",
              "4       the ending portion of these vedas is called up...  <sos> इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता ...\n",
              "...                                                   ...                                                ...\n",
              "127602  examples of art deco construction can be found...  <sos> आर्ट डेको शैली के निर्माण मैरीन ड्राइव औ...\n",
              "127603                           and put it in our cheeks        <sos> और अपने गालों में डाल लेते हैं। <eos>\n",
              "127604  as for the other derivatives of sulphur the co...  <sos> जहां तक गंधक के अन्य उत्पादों का प्रश्न ...\n",
              "127605  its complicated functioning is defined thus in...  <sos> zरचनाप्रकिया को उसने एक पहेली में यों बा...\n",
              "127606  theyve just won four government contracts to b...  <sos> हाल ही में उन्हें सरकारी ठेका मिला है कर...\n",
              "\n",
              "[127607 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcbtEfi3ZOnS",
        "colab_type": "text"
      },
      "source": [
        "##Building Dataset\n",
        "\n",
        "**Note :** As i am not using a attention mechanism in this assignment, the length of the sequences is a factor in determining the accuracy that we will obtain.\n",
        "\n",
        "So we initially restrict the maximum length of each sentence to 15 words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suvtr_ezogmr",
        "colab_type": "code",
        "outputId": "4296cd77-2e7a-4120-e09e-e637ecc4065b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "X, y = [], [] # input and output data\n",
        "maxlen = 15   # maximum length of each sentence\n",
        "for i in range(len(df[\"hi\"])):\n",
        "  temp_hi = df[\"hi\"][i].split()\n",
        "  temp_en = df[\"en\"][i].split()\n",
        "  if(len(temp_en)<=maxlen and len(temp_hi)<=maxlen):\n",
        "    X.append(temp_en)\n",
        "    y.append(temp_hi)\n",
        "\n",
        "len(X) ,X[0], y[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(66853,\n",
              " ['politicians',\n",
              "  'do',\n",
              "  'not',\n",
              "  'have',\n",
              "  'permission',\n",
              "  'to',\n",
              "  'do',\n",
              "  'what',\n",
              "  'needs',\n",
              "  'to',\n",
              "  'be',\n",
              "  'done'],\n",
              " ['<sos>',\n",
              "  'राजनीतिज्ञों',\n",
              "  'के',\n",
              "  'पास',\n",
              "  'जो',\n",
              "  'कार्य',\n",
              "  'करना',\n",
              "  'चाहिए',\n",
              "  'वह',\n",
              "  'करने',\n",
              "  'कि',\n",
              "  'अनुमति',\n",
              "  'नहीं',\n",
              "  'है',\n",
              "  '<eos>'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQQ56eBoam0L",
        "colab_type": "text"
      },
      "source": [
        "##Tokenization\n",
        "\n",
        "we have to encode our dataset with integers so that we can represnt each word by an unique token.\n",
        "\n",
        "Keras provied a good tokenizer so i have implemented using that and also padded the sequences using a keras function as well.\n",
        "\n",
        "initially i had made the mistake of converting the array into float32 type which gave a really poor perfomance in the evaluation, so i have maintained it as default int32 and later it will be changed in the batching function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1wVLGs2GTwY",
        "colab_type": "code",
        "outputId": "51a6be57-3bba-4132-d73b-60e066fc4d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "hi_tok , en_tok = Tokenizer(), Tokenizer()\n",
        "\n",
        "en_tok.fit_on_texts(X)\n",
        "hi_tok.fit_on_texts(y)\n",
        "X_vector = []\n",
        "y_vector = []\n",
        "\n",
        "X_vector = en_tok.texts_to_sequences(X)       #converts the vocabulary into integer indexes\n",
        "y_vector = hi_tok.texts_to_sequences(y)\n",
        "\n",
        "X_vector = tf.keras.preprocessing.sequence.pad_sequences(X_vector, padding='post')  #pad to obtain same sequence length - maxlen\n",
        "y_vector = tf.keras.preprocessing.sequence.pad_sequences(y_vector, padding='post')\n",
        "\n",
        "X_vec_to_word = dict(map(reversed, en_tok.word_index.items()))    #this is the inverese dictionary to map the indexes back to words\n",
        "y_vec_to_word = dict(map(reversed, hi_tok.word_index.items()))\n",
        "\n",
        "hi_vocab = len(hi_tok.word_index.items())+1\n",
        "en_vocab = len(en_tok.word_index.items())\n",
        "\n",
        "# X_vector = X_vector.astype(\"float32\")\n",
        "# y_vector = y_vector.astype(\"float32\")\n",
        "\n",
        "print(en_vocab, hi_vocab)\n",
        "X_vector, X_test, y_vector, y_test = train_test_split(X_vector, y_vector, test_size=0.2)\n",
        "len(X_vector), X_vector, y_vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31420 32740\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(53482, array([[27640,     0,     0, ...,     0,     0,     0],\n",
              "        [    3,    13,   214, ...,     0,     0,     0],\n",
              "        [    8,    15,   521, ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [  615,    31,   779, ...,     0,     0,     0],\n",
              "        [    8,    79,  2288, ...,     0,     0,     0],\n",
              "        [ 2091,  3837,  1095, ...,     0,     0,     0]], dtype=int32), array([[    1,   293,  7288, ...,     0,     0,     0],\n",
              "        [    1,    50,  2962, ...,     0,     0,     0],\n",
              "        [    1,    26,    23, ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [    1,  6417,     8, ..., 17226,     2,     0],\n",
              "        [    1,   297,  3338, ...,     0,     0,     0],\n",
              "        [    1,  6400,  1391, ...,     0,     0,     0]], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa_XXHYGbn3M",
        "colab_type": "text"
      },
      "source": [
        "##Batching for fitting the Model on System\n",
        "\n",
        "This is an **important** learning for me as i had come to use the yield function recently. One of the problems we face is the Over consumption of RAM as some variables take up a lot of space.\n",
        "\n",
        "So our model will output a categorical output (One-Shot), This is a large 3D matrix.\n",
        "\n",
        "Feeding data in batches to the model alone does not solve this problem of RAM consupmtion if we just merely slice values from a big matrix stored in memory, So if we even try storing a \n",
        "\n",
        "- np.zeros((vocabulary_len,maxlen , len(X)))\n",
        "\n",
        "the session crahes due to over consumption of RAM, so 'yield' allows us to return subsets of iterable data which consumes only memory for that batch_size.\n",
        "\n",
        "INITIAL BATCHING FUNCTION I CREATED :\n",
        "\n",
        "```\n",
        "batch_size = 256\n",
        "\n",
        "def batches(X, y, batch_size= 256):\n",
        "  while(True):\n",
        "    data = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    data = data.batch(batch_size, drop_remainder=True)\n",
        "    for i, (inp, out) in enumerate(data):\n",
        "      y_onehot = to_categorical(out[:,1:], num_classes=hi_vocab)\n",
        "      y_onehot[:,:,0] = 0\n",
        "      y_onehot = np.insert(y_onehot,-1, np.zeros(hi_vocab),1)\n",
        "\n",
        "    decoder_input = np.zeros((batch_size, maxlen),dtype='float32')\n",
        "    encoder_input = np.zeros((batch_size, maxlen),dtype='float32')\n",
        "    end_token_value = 2.0000e+00\n",
        "\n",
        "    for i in range(len(out)):\n",
        "      for j in range(len(out[i])):\n",
        "        if(int(out[i, j])!=int(end_token_value)):\n",
        "          decoder_input[i, j] = out[i, j]\n",
        "        encoder_input[i, j] = inp[i, j]\n",
        "      yield([encoder_input, decoder_input], y_onehot)\n",
        "```\n",
        "But this is not very optimized so i have used the batching function from [Reference](https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7)\n",
        "\n",
        "###**NOTE** \n",
        "####I have used the above reference in several places in the code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF4lSz7P4xbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#FROM REFERENCE\n",
        "def generate_batch(X = X_vector, y = y_vector, batch_size = 256):\n",
        "  while(True):\n",
        "    for j in range(0, len(X), batch_size):\n",
        "      encoder_input_data = np.zeros((batch_size, maxlen),dtype='float32')\n",
        "      decoder_input_data = np.zeros((batch_size, maxlen),dtype='float32')\n",
        "      decoder_target_data = np.zeros((batch_size, maxlen, hi_vocab),dtype='float32')\n",
        "      \n",
        "      for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "        \n",
        "        for t, token in enumerate(input_text):\n",
        "          encoder_input_data[i, t] = token # encoder input seq\n",
        "\n",
        "        for t, token in enumerate(target_text):\n",
        "          if t<len(target_text)-1:\n",
        "            decoder_input_data[i, t] = token # decoder input seq\n",
        "          if t>0: #do not include start tag in onehot vector\n",
        "            decoder_target_data[i, t - 1, token] = 1.\n",
        "      yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jqY-Vm0HtQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# en_tok.word_index.items()\n",
        "# batch_size = 256\n",
        "# for i, (input_text, target_text) in enumerate(zip(X_vector[0:0+batch_size], y_vector[0:0+batch_size])):\n",
        "#     for t, word in enumerate(input_text):\n",
        "#       print(i ,t ,word)\n",
        "#     break\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfwYj17V9LOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 256\n",
        "num_encoder_tokens = en_vocab\n",
        "num_decoder_tokens = hi_vocab\n",
        "embedding_dims = 512\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7FQIL8el80Q",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#**Unidirectional LSTM Model**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-BLQzukNLAv",
        "colab_type": "text"
      },
      "source": [
        "##Encoder-Decoder (Teacher Forcing)\n",
        "\n",
        "The Seq to Seq architecture invloves mainly 2 nodes, one LSTM cell in Encoder and one in Decoder. These LSTM cells are however rolled out into several units based on input dimesnions\n",
        "\n",
        "**LSTM OverView:** Each LSTM **outputs 3 vectors**,  one **Cell State**, one **Hidden State**, and **Output**( depending on the dimension, we have defined it as 512. this 512 is nothing but the output of the Embedding layer which has 512 dimensions specified by us)\n",
        "\n",
        "So, the context vector that will be fed to the decoder from the encoder is nothing but the Cell State and Hidden State vectors.\n",
        "\n",
        "**Teacher Forcing** : During Training, we have to give the decoder correct outputs in each timestep to reduce training effort and propogating error. So we initially feed the <SOS> tag to the Decoder PLUS the Cell,Hidden States from the Encoder.\n",
        "\n",
        "At each Timestep whatever the output from decoder maybe, we still feed it with the right word  which is why its called teacher enforcing.\n",
        "\n",
        "**CODING Note :** we have to make return state = true, so that the lstm layer outputs cell,hidden state along with the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-zaQ_wovM7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modified from Reference\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(en_vocab, embedding_dims, mask_zero = True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(embedding_dims, return_state=True, return_sequences=True)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm(enc_emb)\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(hi_vocab, embedding_dims, mask_zero = True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(embedding_dims, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _  = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "\n",
        "decoder_dense = Dense(hi_vocab, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "958WQLrZvofR",
        "colab_type": "code",
        "outputId": "1ee0cdf1-7658-4a11-d304-97b73099980e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "source": [
        "model.summary()\n",
        "encoder_outputs, state_h, state_c,"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 512)    16087040    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 512)    16762880    input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 512),  2099200     embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 512),  2099200     embedding_2[0][0]                \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 32740)  16795620    lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 53,843,940\n",
            "Trainable params: 53,843,940\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor 'lstm_1/transpose_2:0' shape=(None, None, 512) dtype=float32>,\n",
              " <tf.Tensor 'lstm_1/while:5' shape=(None, 512) dtype=float32>,\n",
              " <tf.Tensor 'lstm_1/while:6' shape=(None, 512) dtype=float32>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tas831F3zlo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samples = len(X_vector)\n",
        "val_samples = len(X_test)\n",
        "batch_size = 128          #tried 128, 256\n",
        "epochs = 45               # tried 20,30, 45"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns42bUda_O0Y",
        "colab_type": "code",
        "outputId": "2f3db317-56ef-4d68-bb08-6a8dbdc4ced9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(generator = generate_batch(X_vector, y_vector, batch_size = batch_size),\n",
        "                    steps_per_epoch = train_samples//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
        "                    validation_steps = val_samples//batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45\n",
            "417/417 [==============================] - 172s 413ms/step - loss: 3.7891 - acc: 0.2256 - val_loss: 3.5568 - val_acc: 0.2644\n",
            "Epoch 2/45\n",
            "417/417 [==============================] - 168s 404ms/step - loss: 3.2045 - acc: 0.2784 - val_loss: 2.9642 - val_acc: 0.2898\n",
            "Epoch 3/45\n",
            "417/417 [==============================] - 168s 402ms/step - loss: 2.8976 - acc: 0.3113 - val_loss: 3.0649 - val_acc: 0.3159\n",
            "Epoch 4/45\n",
            "417/417 [==============================] - 168s 402ms/step - loss: 2.6134 - acc: 0.3463 - val_loss: 2.9188 - val_acc: 0.3396\n",
            "Epoch 5/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 2.3379 - acc: 0.3859 - val_loss: 2.8284 - val_acc: 0.3595\n",
            "Epoch 6/45\n",
            "417/417 [==============================] - 167s 400ms/step - loss: 2.0817 - acc: 0.4278 - val_loss: 2.6008 - val_acc: 0.3793\n",
            "Epoch 7/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 1.8526 - acc: 0.4718 - val_loss: 2.7188 - val_acc: 0.3971\n",
            "Epoch 8/45\n",
            "417/417 [==============================] - 167s 400ms/step - loss: 1.6474 - acc: 0.5175 - val_loss: 2.6407 - val_acc: 0.4084\n",
            "Epoch 9/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 1.4638 - acc: 0.5617 - val_loss: 2.4886 - val_acc: 0.4200\n",
            "Epoch 10/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 1.3037 - acc: 0.6025 - val_loss: 2.4374 - val_acc: 0.4266\n",
            "Epoch 11/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 1.1605 - acc: 0.6403 - val_loss: 2.7294 - val_acc: 0.4324\n",
            "Epoch 12/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 1.0330 - acc: 0.6764 - val_loss: 2.8382 - val_acc: 0.4363\n",
            "Epoch 13/45\n",
            "417/417 [==============================] - 168s 402ms/step - loss: 0.9182 - acc: 0.7102 - val_loss: 2.7123 - val_acc: 0.4397\n",
            "Epoch 14/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.8176 - acc: 0.7407 - val_loss: 2.4690 - val_acc: 0.4418\n",
            "Epoch 15/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.7282 - acc: 0.7679 - val_loss: 2.6640 - val_acc: 0.4435\n",
            "Epoch 16/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.6489 - acc: 0.7935 - val_loss: 2.8447 - val_acc: 0.4455\n",
            "Epoch 17/45\n",
            "417/417 [==============================] - 167s 400ms/step - loss: 0.5762 - acc: 0.8169 - val_loss: 2.8039 - val_acc: 0.4445\n",
            "Epoch 18/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.5092 - acc: 0.8386 - val_loss: 2.7967 - val_acc: 0.4406\n",
            "Epoch 19/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.4500 - acc: 0.8585 - val_loss: 3.1598 - val_acc: 0.4434\n",
            "Epoch 20/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.3979 - acc: 0.8766 - val_loss: 2.8553 - val_acc: 0.4449\n",
            "Epoch 21/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.3492 - acc: 0.8936 - val_loss: 2.8672 - val_acc: 0.4423\n",
            "Epoch 22/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.3062 - acc: 0.9085 - val_loss: 2.8880 - val_acc: 0.4391\n",
            "Epoch 23/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.2656 - acc: 0.9231 - val_loss: 3.2038 - val_acc: 0.4395\n",
            "Epoch 24/45\n",
            "417/417 [==============================] - 167s 400ms/step - loss: 0.2300 - acc: 0.9365 - val_loss: 2.5198 - val_acc: 0.4416\n",
            "Epoch 25/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.1983 - acc: 0.9475 - val_loss: 2.9158 - val_acc: 0.4394\n",
            "Epoch 26/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.1713 - acc: 0.9566 - val_loss: 3.1311 - val_acc: 0.4362\n",
            "Epoch 27/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.1488 - acc: 0.9647 - val_loss: 3.1632 - val_acc: 0.4356\n",
            "Epoch 28/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.1296 - acc: 0.9713 - val_loss: 2.8651 - val_acc: 0.4371\n",
            "Epoch 29/45\n",
            "417/417 [==============================] - 168s 402ms/step - loss: 0.1133 - acc: 0.9763 - val_loss: 3.0450 - val_acc: 0.4380\n",
            "Epoch 30/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.1003 - acc: 0.9803 - val_loss: 2.8963 - val_acc: 0.4364\n",
            "Epoch 31/45\n",
            "417/417 [==============================] - 167s 400ms/step - loss: 0.0895 - acc: 0.9833 - val_loss: 3.3944 - val_acc: 0.4380\n",
            "Epoch 32/45\n",
            "417/417 [==============================] - 168s 402ms/step - loss: 0.0804 - acc: 0.9860 - val_loss: 2.8112 - val_acc: 0.4389\n",
            "Epoch 33/45\n",
            "417/417 [==============================] - 168s 402ms/step - loss: 0.0721 - acc: 0.9883 - val_loss: 3.1238 - val_acc: 0.4398\n",
            "Epoch 34/45\n",
            "417/417 [==============================] - 168s 402ms/step - loss: 0.0652 - acc: 0.9897 - val_loss: 3.3344 - val_acc: 0.4401\n",
            "Epoch 35/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.0591 - acc: 0.9912 - val_loss: 3.3004 - val_acc: 0.4395\n",
            "Epoch 36/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.0539 - acc: 0.9923 - val_loss: 3.7191 - val_acc: 0.4398\n",
            "Epoch 37/45\n",
            "417/417 [==============================] - 167s 400ms/step - loss: 0.0495 - acc: 0.9932 - val_loss: 3.3905 - val_acc: 0.4415\n",
            "Epoch 38/45\n",
            "417/417 [==============================] - 167s 400ms/step - loss: 0.0461 - acc: 0.9938 - val_loss: 3.4269 - val_acc: 0.4414\n",
            "Epoch 39/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.0437 - acc: 0.9939 - val_loss: 3.2519 - val_acc: 0.4399\n",
            "Epoch 40/45\n",
            "417/417 [==============================] - 167s 401ms/step - loss: 0.0427 - acc: 0.9940 - val_loss: 3.4534 - val_acc: 0.4371\n",
            "Epoch 41/45\n",
            "417/417 [==============================] - 168s 403ms/step - loss: 0.0425 - acc: 0.9937 - val_loss: 3.6407 - val_acc: 0.4376\n",
            "Epoch 42/45\n",
            "417/417 [==============================] - 168s 402ms/step - loss: 0.0440 - acc: 0.9930 - val_loss: 3.9778 - val_acc: 0.4399\n",
            "Epoch 43/45\n",
            "417/417 [==============================] - 168s 402ms/step - loss: 0.0462 - acc: 0.9918 - val_loss: 3.2922 - val_acc: 0.4396\n",
            "Epoch 44/45\n",
            "417/417 [==============================] - 168s 403ms/step - loss: 0.0465 - acc: 0.9914 - val_loss: 3.2915 - val_acc: 0.4404\n",
            "Epoch 45/45\n",
            "417/417 [==============================] - 169s 405ms/step - loss: 0.0433 - acc: 0.9923 - val_loss: 3.4642 - val_acc: 0.4396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f6beb757b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZOeU_jOZXL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"unidirectional4.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1dHL-ErM6DI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model(\"/content/unidirectional4.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6blbR9fsQyDT",
        "colab_type": "text"
      },
      "source": [
        "## Testing the model\n",
        "\n",
        "The Input values that needs to be fed to the decoder changes in Testing in comparison to Training. As i mentioned earlier for teacher enforcing we input the correct translated words at each timestep, but during testing we feed only the \\<SOS\\> token with the Encoder Cell,Hidden States(which is same as before)\n",
        "\n",
        "So whenever an \\<EOS\\>  tag is encountered he model stops feeding data to the next timestep."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyLTthSQjWyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from Reference\n",
        "\n",
        "# Encoder Model same as before\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h1 = Input(shape=(embedding_dims,))\n",
        "decoder_state_input_c1 = Input(shape=(embedding_dims,))\n",
        "decoder_states_inputs = [decoder_state_input_h1, decoder_state_input_c1]\n",
        "\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "\n",
        "# decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lInqJVF8jW8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modified From Reference\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    \n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "  \n",
        "  target_seq = np.zeros((1,1))\n",
        "  target_seq[0, 0] = hi_tok.word_index['<sos>']\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "  while not stop_condition:\n",
        "    output_tokens, h1, c1 = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # Sample a token\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = y_vec_to_word[sampled_token_index]\n",
        "    decoded_sentence += ' '+sampled_char\n",
        "\n",
        "    # Exit condition: either hit max length\n",
        "    # or find stop character.\n",
        "    if (sampled_char == '<eos>' or\n",
        "        len(decoded_sentence) > 30):\n",
        "        stop_condition = True\n",
        "\n",
        "    # Update the target sequence.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    # Update states\n",
        "    states_value = [h1, c1]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydrtSwXNhEX8",
        "colab_type": "text"
      },
      "source": [
        "##BLEU Evaluation\n",
        "\n",
        "Bleu metric evaluates a score based on the words present in the reference and the model's output sentence. It depends on the quality of the reference texts available as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2Xe0xFHp2j1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def bleu(actual, predicted):\n",
        "  actual = actual.split()\n",
        "  predicted = predicted.split()\n",
        "\n",
        "  bleu_score = sentence_bleu([actual], predicted)\n",
        "  return bleu_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-_RLwnVklzG",
        "colab_type": "text"
      },
      "source": [
        "##Output using Test Data\n",
        "we iterate over the test dataset and output meaning for 10 sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNckXPw6L679",
        "colab_type": "code",
        "outputId": "8e5990e4-a5ff-48b7-f48d-4998afdb858c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "inp = X_test\n",
        "output = y_test\n",
        "for j in range(0,15):\n",
        "  sent= ''\n",
        "  input_seq=np.zeros((1,15))\n",
        "  for i in range(len(inp[j])):\n",
        "    if(inp[j][i] in en_tok.word_index.values()):\n",
        "      input_seq[0, i] = inp[j][i]\n",
        "    # padding = [0.]*(15-len(X_test[j]))\n",
        "  # input_seq += padding\n",
        "\n",
        "\n",
        "  decoded_sentence = decode_sequence(input_seq=input_seq)\n",
        "  if(decoded_sentence[-5:]==\"<eos>\"):\n",
        "    decoded_sentence = decoded_sentence[:-5]\n",
        "  bleuScore = bleu(hi_tok.sequences_to_texts([output[j]])[0][5:-5], decoded_sentence) # [5:-5] is to remove <sos> and <eos> tags\n",
        "\n",
        "\n",
        "  # print(input_seq)\n",
        "  print('\\nInput :',en_tok.sequences_to_texts([inp[j]]))          # converts the integer represented sentence to words\n",
        "  print('Correct Hindi:', hi_tok.sequences_to_texts([output[j]]))\n",
        "  print('Predicted Hindi:', decoded_sentence, \"\\nBLEU Score : \", bleuScore)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Input : ['it was the beginning of a much higher degree']\n",
            "Correct Hindi: ['<sos> यह शुरुआत थी एक बहुत उपरी स्तर <eos>']\n",
            "Predicted Hindi:  ये बहुत महान था ।  \n",
            "BLEU Score :  0.4482700320176827\n",
            "\n",
            "Input : ['taj mahal is unique model of mugal architecture']\n",
            "Correct Hindi: ['<sos> ताज महल मुगल वास्तुकला का उत्कृष्ट नमूना है। <eos>']\n",
            "Predicted Hindi:  ताजमहल महल का एक प्रान्त हैं शहर \n",
            "BLEU Score :  0.6337834876616586\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Input : ['creation of surdan in poetry dictionary']\n",
            "Correct Hindi: ['<sos> सूरदास की रचनाएँ कविताकोश में <eos>']\n",
            "Predicted Hindi:  हरिवंश राय बच्चन की रचनाएँ  \n",
            "BLEU Score :  0.5623413251903491\n",
            "\n",
            "Input : ['i always loved political cartoons']\n",
            "Correct Hindi: ['<sos> मुझे राजनीतिक कार्टून हमेशा बहुत पसंद थे <eos>']\n",
            "Predicted Hindi:  मुझे हमेशा एक खबर हटाई जा रहा है \n",
            "BLEU Score :  0.7071067811865476\n",
            "\n",
            "Input : ['in fact in the united states the teaching system has worked fairly well']\n",
            "Correct Hindi: ['<sos> वास्तव में यूनाइटेड स्टेट्स में शिक्षण प्रणाली ने बहुत अच्छा काम किया है। <eos>']\n",
            "Predicted Hindi:  वास्तव में केवल अमरीका में एक संस्थान \n",
            "BLEU Score :  0.21938936848339244\n",
            "\n",
            "Input : ['news']\n",
            "Correct Hindi: ['<sos> समाचार <eos>']\n",
            "Predicted Hindi:  समाचार  \n",
            "BLEU Score :  1.0\n",
            "\n",
            "Input : ['most young people will never try sniffing']\n",
            "Correct Hindi: ['<sos> अधिकतर युवा लोग सॉल्वैंट लोसूँघने की आज़माइश कभी नहीं करेंगे <eos>']\n",
            "Predicted Hindi:  बहुत से लोग ढेर सारा अच्छे नहीं \n",
            "BLEU Score :  0.476273899703797\n",
            "\n",
            "Input : ['applause']\n",
            "Correct Hindi: ['<sos> ठहाका <eos>']\n",
            "Predicted Hindi:  अभिवादन  \n",
            "BLEU Score :  0\n",
            "\n",
            "Input : ['laughter']\n",
            "Correct Hindi: ['<sos> हंसी <eos>']\n",
            "Predicted Hindi:  हंसी  \n",
            "BLEU Score :  1.0\n",
            "\n",
            "Input : ['and then i started getting excited when it started getting dozens']\n",
            "Correct Hindi: ['<sos> फिर मुझे इसपर दर्ज़नों सैंकड़ों हज़ारों और फिर लाखों हिट्स <eos>']\n",
            "Predicted Hindi:  और फ़िर जब वापस शुरु हुआ वह तो \n",
            "BLEU Score :  0.4630777161991027\n",
            "\n",
            "Input : ['where everyone is collaborating on content']\n",
            "Correct Hindi: ['<sos> जन्हा सभी लोग विषयवस्तु पर सहयोग करते हैं <eos>']\n",
            "Predicted Hindi:  जहाँ सभी लोग बिजली से बिजली दिखाया \n",
            "BLEU Score :  0.40495158902656925\n",
            "\n",
            "Input : ['this feeling is strongest among the illiterate']\n",
            "Correct Hindi: ['<sos> यह भावना अशिक्षितों में ज्यादा प्रबल दिखती है <eos>']\n",
            "Predicted Hindi:  ये एक तरह बैठ गया है  \n",
            "BLEU Score :  0.45782273986766686\n",
            "\n",
            "Input : ['kid “and you thought you could do anything”']\n",
            "Correct Hindi: ['<sos> बच्चा“और तुम्हें लगता था कि तुम जो चाहे कर सकते थे” <eos>']\n",
            "Predicted Hindi:  आप की जान बचाने का सामना करने में \n",
            "BLEU Score :  0\n",
            "\n",
            "Input : ['rama requested to ocean for giving a way']\n",
            "Correct Hindi: ['<sos> राम ने समुद्र से रास्ता देने की विनती की। <eos>']\n",
            "Predicted Hindi:  राम ने समुद्र से रास्ता आरंभ कर \n",
            "BLEU Score :  0.46199933699457096\n",
            "\n",
            "Input : ['to get that device just below the tabletop']\n",
            "Correct Hindi: ['<sos> अपने मोबाईल को टेबल से थोड़ी निचे रखने की <eos>']\n",
            "Predicted Hindi:  जिससे वो सीधे इतना आवश्यक हो जाएं \n",
            "BLEU Score :  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8xl63S3gYSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zQk_rtSyKfb",
        "colab_type": "text"
      },
      "source": [
        "# **Bidirectional LSTM layer :**  \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "[Google's NMT '16](https://arxiv.org/pdf/1609.08144.pdf) paper elaborates on deep lstm network and how the model is improved when we increase the number of layers to a certain limit.\n",
        "\n",
        "Similar to GNMT, i have created a modle with one Bidirectional LSTM in the encoder followed by a normal LSTM layer,\n",
        "\n",
        "And 2 LSTM layers on the Decoder side as you cannot have a Bi-LSTM on the decoder side as we will be predicting sequentially.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HUWP83_1Vb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 512\n",
        "num_encoder_tokens = en_vocab\n",
        "num_decoder_tokens = hi_vocab\n",
        "embedding_dims = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E44Lh9urfk5",
        "colab_type": "text"
      },
      "source": [
        "##Encoder-Decoder\n",
        "\n",
        "**Architecture Diagram:**\n",
        "![alt text](https://github.com/svtsanoj/NMT-English-Hindi/blob/master/NMT.jpeg?raw=true)\n",
        "\n",
        "The Bidirectional LSTM  is the first layer after the usual embedding layer as you can see.\n",
        "\n",
        "A bidirectional LSTM outputs 5 values unlike a normal LSTM which outputs 3, these extra 2 are nothing but the hidden and cell state of the reversed LSTM inside the Bi-LSTM.\n",
        "\n",
        "So if you want to try this with just one Bidirectional layer in the enoder without another LSTM, you'll have to make sure that all the 4 States are fed to the Decoder but this will not be possible as Single layer lstm can only take 2 states as input so we use additional layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHzalI4EyvvC",
        "colab_type": "code",
        "outputId": "8fa35739-ebe6-4d3f-b547-12f7e0774f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "# Encoder\n",
        "\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "\n",
        "enc_emb =  Embedding(en_vocab, embedding_dims, mask_zero = True)(encoder_inputs)\n",
        "\n",
        "encoder_lstm1 = Bidirectional(LSTM(embedding_dims, return_sequences=True))(enc_emb)\n",
        "\n",
        "encoder_lstm2 = LSTM(embedding_dims, return_state=True, return_sequences=True)\n",
        "\n",
        "encoder_outputs, state_h, state_c= encoder_lstm2(encoder_lstm1)\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "dec_emb_layer =Embedding(hi_vocab, embedding_dims, mask_zero = True)    #do not declare as Embedding(..)(input) as this layer will be called later\n",
        "dec_emb = dec_emb_layer(decoder_inputs)                                  \n",
        "\n",
        "decoder_lstm1 =  LSTM(embedding_dims, return_sequences=True)\n",
        "dec_lstm1 = decoder_lstm1(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "decoder_lstm2 =  LSTM(embedding_dims, return_sequences=True, return_state=True)\n",
        "\n",
        "decoder_outputs, _, _  = decoder_lstm2(dec_lstm1)\n",
        "\n",
        "\n",
        "decoder_dense = Dense(hi_vocab, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "encoder_outputs, state_h, state_c,"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 512)    16087040    input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, None, 1024)   4198400     embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, None, 512)    16762880    input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, None, 512),  3147776     bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, None, 512)    2099200     embedding_4[0][0]                \n",
            "                                                                 lstm_4[0][1]                     \n",
            "                                                                 lstm_4[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   [(None, None, 512),  2099200     lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, None, 32740)  16795620    lstm_6[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 61,190,116\n",
            "Trainable params: 61,190,116\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor 'lstm_4/transpose_2:0' shape=(None, None, 512) dtype=float32>,\n",
              " <tf.Tensor 'lstm_4/while:5' shape=(None, 512) dtype=float32>,\n",
              " <tf.Tensor 'lstm_4/while:6' shape=(None, 512) dtype=float32>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0c7e7f27-36cc-43e2-8336-d19dd1fb11c8",
        "id": "uc5798kIaKKi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_samples = len(X_vector)\n",
        "val_samples = len(X_test)\n",
        "batch_size = 128\n",
        "epochs = 45       #25 , 45\n",
        "\n",
        "model.fit_generator(generator = generate_batch(X_vector, y_vector, batch_size = batch_size),\n",
        "                    steps_per_epoch = train_samples//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
        "                    validation_steps = val_samples//batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45\n",
            "417/417 [==============================] - 282s 676ms/step - loss: 3.6285 - acc: 0.2302 - val_loss: 3.2704 - val_acc: 0.2659\n",
            "Epoch 2/45\n",
            "417/417 [==============================] - 283s 679ms/step - loss: 3.2056 - acc: 0.2806 - val_loss: 2.8923 - val_acc: 0.2923\n",
            "Epoch 3/45\n",
            "417/417 [==============================] - 282s 676ms/step - loss: 2.9848 - acc: 0.3062 - val_loss: 3.0108 - val_acc: 0.3131\n",
            "Epoch 4/45\n",
            "417/417 [==============================] - 283s 679ms/step - loss: 2.7860 - acc: 0.3296 - val_loss: 2.8688 - val_acc: 0.3273\n",
            "Epoch 5/45\n",
            "417/417 [==============================] - 283s 678ms/step - loss: 2.6047 - acc: 0.3538 - val_loss: 2.9972 - val_acc: 0.3376\n",
            "Epoch 6/45\n",
            "417/417 [==============================] - 284s 682ms/step - loss: 2.4364 - acc: 0.3798 - val_loss: 3.1040 - val_acc: 0.3510\n",
            "Epoch 7/45\n",
            "417/417 [==============================] - 284s 681ms/step - loss: 2.2814 - acc: 0.4080 - val_loss: 2.9936 - val_acc: 0.3607\n",
            "Epoch 8/45\n",
            "417/417 [==============================] - 283s 679ms/step - loss: 2.1391 - acc: 0.4374 - val_loss: 2.8148 - val_acc: 0.3679\n",
            "Epoch 9/45\n",
            "417/417 [==============================] - 284s 681ms/step - loss: 2.0022 - acc: 0.4661 - val_loss: 2.9298 - val_acc: 0.3763\n",
            "Epoch 10/45\n",
            "417/417 [==============================] - 284s 680ms/step - loss: 1.8774 - acc: 0.4948 - val_loss: 2.7386 - val_acc: 0.3813\n",
            "Epoch 11/45\n",
            "417/417 [==============================] - 283s 679ms/step - loss: 1.7518 - acc: 0.5243 - val_loss: 2.8488 - val_acc: 0.3834\n",
            "Epoch 12/45\n",
            "417/417 [==============================] - 284s 680ms/step - loss: 1.6319 - acc: 0.5541 - val_loss: 2.9708 - val_acc: 0.3855\n",
            "Epoch 13/45\n",
            "417/417 [==============================] - 282s 677ms/step - loss: 1.5203 - acc: 0.5824 - val_loss: 2.7454 - val_acc: 0.3877\n",
            "Epoch 14/45\n",
            "417/417 [==============================] - 283s 678ms/step - loss: 1.4140 - acc: 0.6106 - val_loss: 2.7501 - val_acc: 0.3892\n",
            "Epoch 15/45\n",
            "417/417 [==============================] - 284s 681ms/step - loss: 1.3159 - acc: 0.6371 - val_loss: 2.7674 - val_acc: 0.3917\n",
            "Epoch 16/45\n",
            "417/417 [==============================] - 282s 677ms/step - loss: 1.2220 - acc: 0.6643 - val_loss: 3.0751 - val_acc: 0.3915\n",
            "Epoch 17/45\n",
            "417/417 [==============================] - 283s 678ms/step - loss: 1.1386 - acc: 0.6892 - val_loss: 2.6395 - val_acc: 0.3905\n",
            "Epoch 18/45\n",
            "417/417 [==============================] - 282s 677ms/step - loss: 1.0638 - acc: 0.7125 - val_loss: 3.1880 - val_acc: 0.3893\n",
            "Epoch 19/45\n",
            "417/417 [==============================] - 284s 680ms/step - loss: 0.9943 - acc: 0.7344 - val_loss: 2.8518 - val_acc: 0.3891\n",
            "Epoch 20/45\n",
            "417/417 [==============================] - 283s 678ms/step - loss: 0.9275 - acc: 0.7553 - val_loss: 3.0607 - val_acc: 0.3873\n",
            "Epoch 21/45\n",
            "417/417 [==============================] - 283s 679ms/step - loss: 0.8611 - acc: 0.7747 - val_loss: 2.9508 - val_acc: 0.3880\n",
            "Epoch 22/45\n",
            "417/417 [==============================] - 283s 679ms/step - loss: 0.8011 - acc: 0.7938 - val_loss: 2.9439 - val_acc: 0.3871\n",
            "Epoch 23/45\n",
            "417/417 [==============================] - 285s 682ms/step - loss: 0.7492 - acc: 0.8087 - val_loss: 2.9603 - val_acc: 0.3883\n",
            "Epoch 24/45\n",
            "417/417 [==============================] - 284s 681ms/step - loss: 0.7006 - acc: 0.8236 - val_loss: 3.0918 - val_acc: 0.3861\n",
            "Epoch 25/45\n",
            "417/417 [==============================] - 286s 686ms/step - loss: 0.6584 - acc: 0.8359 - val_loss: 3.2117 - val_acc: 0.3846\n",
            "Epoch 26/45\n",
            "417/417 [==============================] - 286s 685ms/step - loss: 0.6197 - acc: 0.8474 - val_loss: 3.0223 - val_acc: 0.3838\n",
            "Epoch 27/45\n",
            "417/417 [==============================] - 283s 678ms/step - loss: 0.5838 - acc: 0.8581 - val_loss: 3.2673 - val_acc: 0.3837\n",
            "Epoch 28/45\n",
            "417/417 [==============================] - 283s 680ms/step - loss: 0.5520 - acc: 0.8678 - val_loss: 3.2446 - val_acc: 0.3838\n",
            "Epoch 29/45\n",
            "417/417 [==============================] - 283s 680ms/step - loss: 0.5223 - acc: 0.8763 - val_loss: 3.0099 - val_acc: 0.3842\n",
            "Epoch 30/45\n",
            "417/417 [==============================] - 284s 681ms/step - loss: 0.4950 - acc: 0.8833 - val_loss: 3.1983 - val_acc: 0.3860\n",
            "Epoch 31/45\n",
            "417/417 [==============================] - 283s 679ms/step - loss: 0.4689 - acc: 0.8900 - val_loss: 3.8318 - val_acc: 0.3860\n",
            "Epoch 32/45\n",
            "417/417 [==============================] - 283s 678ms/step - loss: 0.4452 - acc: 0.8960 - val_loss: 3.6110 - val_acc: 0.3833\n",
            "Epoch 33/45\n",
            "417/417 [==============================] - 283s 679ms/step - loss: 0.4219 - acc: 0.9020 - val_loss: 3.4936 - val_acc: 0.3866\n",
            "Epoch 34/45\n",
            "417/417 [==============================] - 283s 678ms/step - loss: 0.3998 - acc: 0.9075 - val_loss: 3.6551 - val_acc: 0.3866\n",
            "Epoch 35/45\n",
            "417/417 [==============================] - 283s 678ms/step - loss: 0.3826 - acc: 0.9114 - val_loss: 3.5869 - val_acc: 0.3859\n",
            "Epoch 36/45\n",
            "417/417 [==============================] - 282s 676ms/step - loss: 0.3631 - acc: 0.9163 - val_loss: 3.1493 - val_acc: 0.3870\n",
            "Epoch 37/45\n",
            "417/417 [==============================] - 282s 677ms/step - loss: 0.3455 - acc: 0.9203 - val_loss: 3.3801 - val_acc: 0.3857\n",
            "Epoch 38/45\n",
            "417/417 [==============================] - 282s 676ms/step - loss: 0.3298 - acc: 0.9235 - val_loss: 3.4924 - val_acc: 0.3862\n",
            "Epoch 39/45\n",
            "417/417 [==============================] - 283s 678ms/step - loss: 0.3144 - acc: 0.9268 - val_loss: 3.4573 - val_acc: 0.3866\n",
            "Epoch 40/45\n",
            "417/417 [==============================] - 283s 678ms/step - loss: 0.3000 - acc: 0.9302 - val_loss: 3.2795 - val_acc: 0.3868\n",
            "Epoch 41/45\n",
            "417/417 [==============================] - 283s 679ms/step - loss: 0.2874 - acc: 0.9325 - val_loss: 3.1591 - val_acc: 0.3853\n",
            "Epoch 42/45\n",
            "417/417 [==============================] - 282s 677ms/step - loss: 0.2747 - acc: 0.9354 - val_loss: 3.2892 - val_acc: 0.3862\n",
            "Epoch 43/45\n",
            "417/417 [==============================] - 282s 676ms/step - loss: 0.2619 - acc: 0.9384 - val_loss: 3.1454 - val_acc: 0.3859\n",
            "Epoch 44/45\n",
            "417/417 [==============================] - 282s 677ms/step - loss: 0.2520 - acc: 0.9398 - val_loss: 3.5497 - val_acc: 0.3854\n",
            "Epoch 45/45\n",
            "417/417 [==============================] - 283s 679ms/step - loss: 0.2413 - acc: 0.9423 - val_loss: 3.0617 - val_acc: 0.3852\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fa8053fccf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqT89ZVp0vID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"bidirectional5.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR9cwWp4fnBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model(\"/content/bidirectional5.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXMoWDGW3zxj",
        "colab_type": "text"
      },
      "source": [
        "##Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqtvevVx-X9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h1 = Input(shape=(embedding_dims,))\n",
        "decoder_state_input_c1 = Input(shape=(embedding_dims,))\n",
        "decoder_states_inputs = [decoder_state_input_h1, decoder_state_input_c1]\n",
        "\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "dec_lstm3 = decoder_lstm1(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm2(dec_lstm3)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "\n",
        "# decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yivZYTY-dN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    \n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1,1))    # Generate empty target sequence of length 1.\n",
        "    target_seq[0, 0] = hi_tok.word_index['<sos>']   # Populate the first character of target sequence with the start character.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h1, c1 = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = y_vec_to_word[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        if (sampled_char == '<eos>' or\n",
        "           len(decoded_sentence) > 30):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1,1))     # Update the target sequence\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        states_value = [h1, c1]   # Update states\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrgBXA9v37Qu",
        "colab_type": "text"
      },
      "source": [
        "##Output using Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "655e67cd-89ef-46d7-dd00-4f5eae946833",
        "id": "NdDCrB8uacfV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "inp = X_vector\n",
        "output = y_vector\n",
        "\n",
        "for j in range(0,20):\n",
        "  sent= ''\n",
        "  input_seq=np.zeros((1,15))\n",
        "  for i in range(len(inp[j])):\n",
        "    if(inp[j][i] in en_tok.word_index.values()):\n",
        "      input_seq[0, i] = inp[j][i]\n",
        "    # padding = [0.]*(15-len(X_test[j]))\n",
        "  # input_seq += padding\n",
        "\n",
        "  decoded_sentence = decode_sequence(input_seq=input_seq)\n",
        "  if(decoded_sentence[-5:]==\"<eos>\"):\n",
        "    decoded_sentence = decoded_sentence[:-5]\n",
        "  bleuScore = bleu(hi_tok.sequences_to_texts([output[j]])[0][5:-5], decoded_sentence) # [5:-5] is to remove <sos> and <eos> tags\n",
        "\n",
        "  # print(input_seq)\n",
        "  print('\\nInput :',en_tok.sequences_to_texts([inp[j]]))\n",
        "  print('Correct Hindi:', hi_tok.sequences_to_texts([output[j]]))\n",
        "  print('Predicted Hindi:', decoded_sentence, \"\\nBLEU Score : \", bleuScore)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Input : ['and learn that as a foreign language']\n",
            "Correct Hindi: ['<sos> और उसे विदेशी भाषा के रूप में सिखा <eos>']\n",
            "Predicted Hindi:  और में डिग्री असलियत से देखिये \n",
            "BLEU Score :  0.5444460596606694\n",
            "\n",
            "Input : ['electronic communication']\n",
            "Correct Hindi: ['<sos> इलेक्ट्रॉनिक संचार <eos>']\n",
            "Predicted Hindi:  तथा यही जहां  \n",
            "BLEU Score :  0\n",
            "\n",
            "Input : ['where do we come up with these ideas']\n",
            "Correct Hindi: ['<sos> ऐसे विचार हमे कहाँ से आते है <eos>']\n",
            "Predicted Hindi:  हम  \n",
            "BLEU Score :  0\n",
            "\n",
            "Input : ['rajya pal is the leader of the statechief minister']\n",
            "Correct Hindi: ['<sos> राज्यपाल राज्य का प्रमुख है। <eos>']\n",
            "Predicted Hindi:  राज्यपाल  \n",
            "BLEU Score :  0.01831563888873418\n",
            "\n",
            "Input : ['and theres kind of a bifurcation']\n",
            "Correct Hindi: ['<sos> और यहाँ एक प्रकार का द्वि विभाजन है <eos>']\n",
            "Predicted Hindi:  और उसका इसने के उसने एक अचानक कीजिये \n",
            "BLEU Score :  0.7071067811865476\n",
            "\n",
            "Input : ['and the fresher the clay the better the impression']\n",
            "Correct Hindi: ['<sos> और मिट्टीं जितनी ही ताजा होगी प्रभाव भी उतना ही अद्ऊण्श्छ्ष्अंभुत होगा <eos>']\n",
            "Predicted Hindi:  और यहाँ और हंसी  \n",
            "BLEU Score :  0.09569649651041094\n",
            "\n",
            "Input : ['and later i can find a wall anywhere']\n",
            "Correct Hindi: ['<sos> और बाद में कहीं भी किसी भी दीवार पर <eos>']\n",
            "Predicted Hindi:  और हूं इच्छा उत्सुक शहरों सकता \n",
            "BLEU Score :  0.38753858253732953\n",
            "\n",
            "Input : ['you must have taken tea in the morning']\n",
            "Correct Hindi: ['<sos> आपने आज सुबह चाय तो पी ही होगी । <eos>']\n",
            "Predicted Hindi:  आपने इस के उसने एक अचानक देखना \n",
            "BLEU Score :  0.46199933699457096\n",
            "\n",
            "Input : ['sometimes there can be temporary anxiety or mild hallucinations']\n",
            "Correct Hindi: ['<sos> कभी कभी अस्थायी चिंता और हल्के मतिभ्रम भी होते हैं <eos>']\n",
            "Predicted Hindi:  कभी यह क्युंकि एक इसने एक अचानक \n",
            "BLEU Score :  0.4004970149398301\n",
            "\n",
            "Input : ['in custody to a domestic government or opposition forces']\n",
            "Correct Hindi: ['<sos> एक घरेलू सरकार या कब्जे में करने के लिए विपक्ष सेना <eos>']\n",
            "Predicted Hindi:  एक मैने एक इसने एक अचानक देखना \n",
            "BLEU Score :  0.34718201116725705\n",
            "\n",
            "Input : ['to all human beings so that they can']\n",
            "Correct Hindi: ['<sos> हर व्यक्ति तक ले जा पाएँगे जिससे कि वो <eos>']\n",
            "Predicted Hindi:  हर उनमें एक इसने एक इसने एक अचानक \n",
            "BLEU Score :  0.5247357977607321\n",
            "\n",
            "Input : ['the first four vedas were composed which was first rigveda']\n",
            "Correct Hindi: ['<sos> पहले चार वेद रचे गये जिनमें ऋग्वेद प्रथम था। <eos>']\n",
            "Predicted Hindi:  पहले  \n",
            "BLEU Score :  0.00033546262790251185\n",
            "\n",
            "Input : ['we kids still dream about perfection']\n",
            "Correct Hindi: ['<sos> हम बच्चे अभी भी पूर्णता की कल्पना कर सकते हैं <eos>']\n",
            "Predicted Hindi:  हम उन्हें जिनका जबकि एक इसने एक \n",
            "BLEU Score :  0.4004970149398301\n",
            "\n",
            "Input : ['my students to delegates to the world trade organization']\n",
            "Correct Hindi: ['<sos> विश्व व्यापार संगठन के प्रतिनिधियों को अपने छात्रों <eos>']\n",
            "Predicted Hindi:  विश्व उचित पड़ी प्रकार शायद उनमें \n",
            "BLEU Score :  0.45782273986766686\n",
            "\n",
            "Input : ['what was the pattern of violence during the previous turbulent phase of the ayodhya movement']\n",
            "Correct Hindi: ['<sos> अयोध्या आंदोलन के पिछले उथलपुथल भरे दौर में किस तरह की हिंसा ही <eos>']\n",
            "Predicted Hindi:  की निकल उनमे ही जहां स्थानों से \n",
            "BLEU Score :  0.310263420349682\n",
            "\n",
            "Input : ['it was such a beautiful machine']\n",
            "Correct Hindi: ['<sos> यह बहुत उत्कृष्ट मशीन थी <eos>']\n",
            "Predicted Hindi:  यह उनमें रखता मित्रों से ये जहां \n",
            "BLEU Score :  0.6147881529512643\n",
            "\n",
            "Input : ['sachin tendulkar is bothhanded']\n",
            "Correct Hindi: ['<sos> सचिन तेंडुलकर उभयहस्त हैं। <eos>']\n",
            "Predicted Hindi:  सचिन से अब ही जहां एक इसने एक अचानक \n",
            "BLEU Score :  0.5773502691896257\n",
            "\n",
            "Input : ['external links']\n",
            "Correct Hindi: ['<sos> बाहरी कडियाँ <eos>']\n",
            "Predicted Hindi:  बाहरी ढूँढने कंपनी  \n",
            "BLEU Score :  0.7598356856515925\n",
            "\n",
            "Input : ['and multiply it by the passion that you have for']\n",
            "Correct Hindi: ['<sos> और इसे अपने उस उत्साह से गुना कीजिये जो <eos>']\n",
            "Predicted Hindi:  और है इच्छा एक तभी एक इसने एक अचानक \n",
            "BLEU Score :  0.5773502691896257\n",
            "\n",
            "Input : ['an idea that art should live in a hermetic bubble']\n",
            "Correct Hindi: ['<sos> और ये कि कला को तो सन्यासियों की दुनिया मे रहना चाहिये <eos>']\n",
            "Predicted Hindi:  और ये जबकि  \n",
            "BLEU Score :  0.037829991229878616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCoRzTUpOCIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "  inp_sentence = re.sub(\"[!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~0-9०-९]\", \"\", str(inp_sentence)).lower()\n",
        "  inp_sentence = re.sub(\"\\s+\", \" \", inp_sentence) \n",
        "  \n",
        "  out_sentence = ''\n",
        "  input_seq=np.zeros((1,15))\n",
        "  for i, word in enumerate(input_sentence.split()):\n",
        "    if(word in en_tok.word_index.keys()):\n",
        "      input_seq[0, i] = en_tok.word_index[word]\n",
        "\n",
        "  decoded_sentence = decode_sequence(input_seq=input_seq)\n",
        "\n",
        "  print('Input :',en_tok.sequences_to_texts([inp[j]]))\n",
        "  print('Predicted :', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VOOnFirDkbL",
        "colab_type": "text"
      },
      "source": [
        "#**Inference**\n",
        "\n",
        "The models seem to perform quite well on even the test data although the bidirectional model does not output as expected. Initially when trained for 25 epochs the result was not good. \n",
        "\n",
        "We can observe that the output sequences are shorter than the average input length. Adding Attention would improve the quality of translation significantly espeacially on long input sentences.\n",
        "\n",
        "References:\n",
        "\n",
        "[1] [Google NMT 2016](https://arxiv.org/pdf/1609.08144.pdf)\n",
        "\n",
        "[2] [Sequence to sequence translation blog](https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7)\n",
        "\n",
        "[3] [Keras blog](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)\n",
        "\n",
        "[4]  [NLTK BLEU](https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.SmoothingFunction)\n",
        "\n",
        "[5] [Intro to BLEU](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)\n",
        "\n",
        "\n"
      ]
    }
  ]
}